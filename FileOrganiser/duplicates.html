<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>duplicates API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>duplicates</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Import packages 

from pathlib import PurePath
import numpy as np
import time
from hurry.filesize import size, alternative
from iteration_utilities import unique_everseen, duplicates
import hashlib
import itertools

# Local imports

from src.fileorganiser.general import *

from src.fileorganiser.tree import *


# ======================================================== #
# ======================== GENERAL ======================= #
# ======================================================== #


# Stores the stats of each individual set of duplicate files - construct by passing file fingerprint + dict
class DupeFpLog():
    &#39;&#39;&#39;
    Class:
    ------

    This class contains the information about a single duplicate fingerprint. Contains the fingerprint, a list of all 
    files that have this fingerprint and the corresponding file sizes (+ duplicated storage occupied) 
    &#39;&#39;&#39;
    
    def __init__(self, Fp, log):
        &#39;&#39;&#39; 
        Description
        -----------
        Constructor for the DupeFpLog class that takes the duplicated fingerprint string and a reference to the main log object.
        Stores a list of all files with this duplicated fingerprint.
        Args:
        -----
            fp : str
                #: The duplicated fingerprint
            log : Log
            #: The log object for the duplicate search

        Outputs
        -------
            allSizeExtClusters: list[list[File]]
                #: List containing all clusters
        &#39;&#39;&#39;
        # Reference to master log set after construction
        self.log = log
        # Fingerprint associated to this log
        self.Fp = Fp
        # List of duplicated files
        self.dupeFiles = log.FpFilesDict[Fp]
        # Maximal subtree root
        #self.commonRoot = findCommonRoot(self.dupeFiles)
        # Filesize and duplicated storage amount
        self.fileSize, self.dupedStorage = CalculateDuplicatedStorage(self.dupeFiles)


# ======================================================== #
# ==================== SIZE CLUSTERING =================== #
# ======================================================== #


def ClusterAllFiles(files:list[File], sizeThreshold):
    &#39;&#39;&#39; 
    Description
    -----------
    Cluster the list of files based on the specified threshold. Then further cluster within each cluster based on file extensions.
    This reduced the number of fingerprints required to be calculated - since only files that are identical in size and extension could ever be duplicates.   
    
    Args:
    -----
        files : list[File or pathlike]
            #: The list of files (or pathlike file objects) to cluster
        sizeThreshold : int
            #: The size of the maximum difference between sequential elements in each cluster (in bytes)


    Outputs
    -------
        allSizeExtClusters: list[list[File]]
            #: List containing all clusters
    &#39;&#39;&#39;
    
    # Cluster all files by size
    fileClusters = ClusterBySize(files, sizeThreshold)
    
    allSizeExtClusters = list()
    for cluster in fileClusters:
        # Further cluster the list of files by extension
        extClusters = ClusterByExtension(cluster)
        for extCluster in extClusters:
            # Append the list of size + extension clustered files 
            allSizeExtClusters.append(extCluster[1])
    
    return allSizeExtClusters

# Cluster files by size based on the threshold (bytes)
def ClusterBySize(files:list, threshold=1000):
    &#39;&#39;&#39; 
    Description
    -----------
    Cluster the list of files based on the specified threshold. Files are clustered sequentially - i.e files are added to the cluster if the difference between the next - previous file is 
    less than the threshold.   
    
    Args:
    -----
        files : list[File or pathlike]
            #: The list of files (or pathlike file objects) to cluster
        threshold : int
            #: The size of the maximum difference between sequential elements in each cluster (in bytes)


    Outputs
    -------
        fileClusters: list[list[File]]
            #: List containing all size clusters 
    &#39;&#39;&#39;
    # Ensure list contains fobjects only
    files = [ParseFobject(file) for file in files]
    # Sort the list of files in size order (descending - using the sortbySizeDesc method)
    sortedFiles = sortbySizeDesc(files)
    sizes = [file.netSize for file in sortedFiles]
    # Cluster the sorted list of files - generator yields (cluster, corresponding indexes from sizes list)
    clusters = list(SizeClusterGen(sizes, threshold))

    # Create a list containing each set of clustered files 
    fileClusters = list()
    for cluster in clusters:
        # Grab the files from the list using the clustered indices
        fileCluster = [files[idx] for idx in cluster[1]]
        fileClusters.append(fileCluster)
    
    return fileClusters

# Generator to yield clustered sizes 
def SizeClusterGen(iterable, threshold):
    &#39;&#39;&#39; 
    Description
    -----------
    Generator that yields the clusters of files (based on size) within the specified threshold. Adapted from https://stackoverflow.com/a/15801233 
    Iteratively adds the next element in the list of sizes if the difference between next - previous is less than the threshold.
    Possible that this can generate large clusters if all sequential files are within the threshold - i.e 1000, 1999, 2998, 3997, 4997 ... would all fall within the same cluster with threshold 1000.
    In practice, a threshold of 1000bytes is sufficient to avoid this possibility with real file sizes ranging in the kilo to megabyte range.  
    
    Args:
    -----
        iterable : list[int]
            #: The list of file sizes to cluster
        threshold : int
            #: The size of the maximum difference between sequential elements in each cluster (in bytes)


    Outputs
    -------
        cluster : list[int]
            #: List of all sizes within the cluster
        clusterIdx : int
            #: The index in the original unclustered list of files that correspond to each clustered size (for matching size to file) 
    &#39;&#39;&#39;
    
    # Initialise at inf so the first difference calculated is always ignored 
    prev = float(&#34;inf&#34;)
    # The clustered set of sizes
    cluster = []
    # The index in the original list of each item in the cluster
    clusterIdx = []

    for idx, item in enumerate(iterable):
        # Check the absolute difference between the previous item (in desc order so diff always &lt;= 0)
        if np.absolute(item - prev) &lt;= threshold:
            
            # Record the item and original list index if difference below threshold
            cluster.append(item)
            clusterIdx.append(idx)
            
        else:
            if len(cluster) &gt; 1 and len(clusterIdx) &gt; 1:
                yield cluster, clusterIdx
            # Create a new cluster starting with this item
            cluster = [item]
            clusterIdx = [idx]
            
        # Move to the next item
        prev = item
    # Yield the current cluster if reached end of iterable
    if len(cluster) &gt; 1 and len(clusterIdx) &gt; 1:
        yield cluster, clusterIdx


# Cluster a list of files based on their extension
def ClusterByExtension(files:list[File]):
    &#39;&#39;&#39; 
    Description
    -----------
    Further cluster files (after size clustering) based on their extension. Any file with an extension that appears only once is dropped. 
    
    
    Args:
    -----
        files : list[File]
            #: The list of files to cluster


    Outputs
    -------
        extensionClusters: list[list[File]]
            #: List containing all extension clusters (each cluster is a list of files with that extension)
    &#39;&#39;&#39;  
    extensions = [file.extension for file in files]
    # List which extensions are present 
    uniqueExt = unique_everseen(extensions)
    # List to store (ext, cluster)
    extensionClusters = list()
    
    for checkExt in uniqueExt:
        extCluster = list()
        # Add to cluster if extensions match
        for idx, ext in enumerate(extensions):
            if(ext == checkExt):
                extCluster.append(files[idx])
        # Only include clusters with more than one file
        if(len(extCluster) &gt; 1):
            extensionClusters.append((checkExt, extCluster))

    return extensionClusters

# ======================================================== #
# ==================== FINGERPRINTING ==================== #
# ======================================================== #

# Fingerprint individual file

def fingerprintFile(file, log):
    &#39;&#39;&#39; 
    Description
    -----------
    Fingerprint an individual file by calculating its 128-bit MD5 hash (represented as a 32-byte hex string). Fingerprints are stored on each file object
    and within the log.FpFilesDict which stores the fingerprint -&gt; files with that fingerprint.
    
    
    Args:
    -----
        file : File
            #: The file to fingerprint
        log : Log
            #: The log object for the duplicate search

    Outputs
    -------
        fileHash: str
            #: 32-byte hex string of the file fingerprint
    &#39;&#39;&#39;  
    
    # Chunksize used in MD5 hash
    chunkSize = 8192
    
    try: 
        # Check if file below threshold size
        if(file.netSize &lt; log.config.sizeThreshold):
            #print(&#34;File below threshold, ignoring&#34;)
            log.filesIgnored += 1
            return None

        with open(ParsePath(file), &#34;rb&#34;) as f:
            fileHash = hashlib.md5()
            while chunk := f.read(chunkSize):
                fileHash.update(chunk)
    except PermissionError:
        print(&#34;Permission error on file: &#34; + str(file.path))
        return None
    except:
        print(&#34;Exception encountered - skipping file: &#34; + str(file.path))
        return None
    
    return fileHash


# Fingerprint all of the files in a folder - store fingerprint string on each file AND in passed fingerprintDict
def fingerprintFolder(folder, log):
    &#39;&#39;&#39; 
    Description
    -----------
    Fingerprint all files within the toplevel of the given folder. Fingerprints are stored on each file object
    and within the log.FpFilesDict which stores the fingerprint -&gt; files with that fingerprint.
    
    
    Args:
    -----
        folder : Folder
            #: The folder to fingerprint files within
        log : Log
            #: The log object for the duplicate search

    Outputs
    -------
        fpList: list[str]
            #: List of fingerprint strings within this folder
    &#39;&#39;&#39;  

    #print(&#34;Fingerprinting folder: &#34; + folder.path.name)
    
    fpList = list()
    #log_storage(folder)
    for f in folder.files:
        fp = fingerprintFile(f, log)
        if fp != None:
            fpString = fp.hexdigest()
            f.fingerprint = fpString
            fpList.append(fpString)
            # Add to fingerprint -&gt; files dictionary (check if already added same fingerprint)
            if(fpString in log.FpFilesDict):
                # Dict already contains this fingerprint, append file to file list
                log.FpFilesDict[fpString].extend({f})
            else:
                # Add fingerprint to dict (key value must be a list of files - for duplicate fingerprints)
                log.FpFilesDict[fpString] = list({f})
        
    return fpList



# Fingerprint every file given a list of folders
# MUST be list of Folder objects, not paths
# FIXME make accept generic path or fobject 

def FingerprintAllFolders(folders:list[Folder], log:Log):
    &#39;&#39;&#39; 
    Description
    -----------
    Fingerprint all files within the given list of folders (the toplevel of each folder only)
    
    
    Args:
    -----
        folders : list[Folder]
            #: The list of all folders (subdirectories included as separate folders in the list) to fingerprint files within
        log : Log
            #: The log object for the duplicate search

    Outputs
    -------
        log: log
            #: Results are stored in the log object
    &#39;&#39;&#39;  

    print(&#34;Fingerprinting all folders: &#34;)
    
    # Scan each folder
    for idx, folder in enumerate(folders):
        # Fingerprint each file in the folder
        folderFpList = fingerprintFolder(folder, log)
        log.FpList.extend(folderFpList)
        log.elapsedTime = time.time()-log.startTime
        print(&#34;Remaining: &#34; + str(len(folders) - idx) + &#34; Elapsed: &#34; + f&#34;{round(log.elapsedTime,1)}&#34; +&#34;s = &#34; + f&#34;{round(log.elapsedTime/60,1)}&#34; + &#34;m &#34; + &#34;  ----  Fingerprinting folder: &#34; + folder.path.as_posix())
        # Record storage size 
        for file in folder.files:
            log.totScannedStorage += file.path.stat().st_size
    return log


def FingerprintAllFiles(files:list[File], log:Log):

    &#39;&#39;&#39; 
    Description
    -----------
    Given a list of files calculate the fingerprint (MD5 hash) for each file. Fingerprints are stored on each file object
    and within the log.FpFilesDict which stores the fingerprint -&gt; files with that fingerprint.
    
    
    Args:
    -----
        files : list[File]
            #: The list of all files to be fingerprinted
        log : Log
            #: The log object for the duplicate search

    Outputs
    -------
    None
        #: Results are stored in the log object
    &#39;&#39;&#39;  

    fpList = list()
    for idx, file in enumerate(files):
        # Ensure passed a fobject
        file = ParseFobject(file)
        # Fingerprint the file
        fp = fingerprintFile(file, log)
        if fp != None:
            fpString = fp.hexdigest()
            file.fingerprint = fpString
            fpList.append(fpString)
            # Add to fingerprint -&gt; files dictionary (check if already added same fingerprint)
            if(fpString in log.FpFilesDict):
                # Dict already contains this fingerprint, append file to file list
                log.FpFilesDict[fpString].append(file)
            else:
                # Add fingerprint to dict (key value must be a list of files - for duplicate fingerprints)
                log.FpFilesDict[fpString] = list({file})
            # Record size in total size scanned
            log.totScannedStorage += file.path.stat().st_size
        # Show elapsed time in console     
        log.elapsedTime = time.time()-log.startTime
        print(&#34;Remaining: &#34; + str(len(files) - idx) + &#34; Elapsed: &#34; + f&#34;{round(log.elapsedTime,1)}&#34; +&#34;s / &#34; + f&#34;{round(log.elapsedTime/60,1)}&#34; + &#34;m &#34; + &#34;  ----  Fingerprinting file: &#34; + str(file.path.as_posix()))
    # Add all fingerprints to log
    log.FpList.extend(fpList)



# Creates the dupeFpLogs given a list of all file fingerprints
def find_duplicate_fingerprints(log):
    &#39;&#39;&#39; 
    Description
    -----------
    Given a list of all fingerprints this identifies any that appear more than once. A DupeFpLog instance is 
    constructed for any duplicated fingerprints, using the fingerprintFiles dict to map 
    the duplicated fingerprint to the corresponding files 
    
    
    Args:
    -----
        log : Log
            #: The log object for this search run

    Outputs
    -------
    
        log : Log
            #: The log with populated results
    &#39;&#39;&#39;  
    # Find duplicated fingerprints
    log.dupeFpList = list(unique_everseen(duplicates(log.FpList)))

    # Create duplicate logs for this fingerprint (constructs folder pair scores if appropriate)
    for dupeFp in log.dupeFpList:
            FPlog = DupeFpLog(dupeFp, log)
            log.dupeFpLogList.append(FPlog)
            log.totDupeStorage += FPlog.dupedStorage
    
    
    
    log.elapsedTime = time.time()-log.startTime
    return log





# ======================================================== #
# ================== SORTING + UTILITIES ================= #
# ======================================================== #


# Sort the list of duplicated fingerprints by descending filesize
&#39;&#39;&#39; &#39;&#39;&#39; 
def OrderDupeStorage(log):
    &#34;&#34;&#34;
        Description:
        ------------
        Sorts the DupeFpLogs based on the duplicated storage size
        Args:
        ----- 
            log : Log
                #: The Log containing the duplicate results

        
        Outputs:
        --------
            sorted : list[DupeFpLog]
                #: The sorted list of duplicate logs

    &#34;&#34;&#34;
    
    dupeSizesUnsorted = []
    sortedDupeLogList = list()
    
    # Read the duplicated storage amounts for each dupeLog
    for dupeFpLog in log.dupeFpLogList:
        dupeSizesUnsorted.extend({dupeFpLog.dupedStorage})
    
    # Flip the list to sort in descending order
    sortedSizeIndices = np.flip((np.argsort(dupeSizesUnsorted)))
    
    # Populate the sorted list of paths
    for unsortIdx in sortedSizeIndices:
        sortedDupeLogList.extend({log.dupeFpLogList[unsortIdx]})
    
    return sortedDupeLogList


# Calculate excess storage taken by duplicates (storage gain from deleting duplicates) 
def CalculateDuplicatedStorage(dupeFiles):
    # Need first file only (all duplicates will have the same size)
    fileSize = dupeFiles[0].netSize
    dupedStorage = fileSize * (len(dupeFiles) - 1)
    return fileSize, dupedStorage


# Replace duplicate files with a symbolic link - keeps file with shortest parent intact
# CURRENTLY UNUSED # 
# def linkDuplicatedFiles(dupeFiles):
#     parents = list()
#     lengths = []
#     for idx, f in enumerate(dupeFiles):
#         parents.append(f.path.parent)
#         lengths.extend({len(str(f.path.parent))})
#     fileToKeep = dupeFiles[np.argmin(lengths)]
#     filesToLink = [f for f in dupeFiles if f != fileToKeep]
#     for f in filesToLink:
#         f.path.symlink_to(fileToKeep.path)
#     return fileToKeep


# def createAllLinks(log):
#     for Fp in log.dupeFpList:
#         dupeFiles = log.FpFilesDict[Fp]
#         fileToKeep = linkDuplicatedFiles(dupeFiles)
#         print(&#34;Linked all duplicates to: &#34; + str(fileToKeep.path.absolute()))


# ======================================================== #
# ==================== COMMON FOLDERS ==================== #
# ======================================================== #


&#39;&#39;&#39;
Given a list of duplicate files, compare their parent folders to obtain similarity score.
Similarity score based on number of duplicated files present in both folders.
&#39;&#39;&#39;
    

# Sort the pair scores in descending order
def SortPairScores(log:Log):
    &#34;&#34;&#34;
        Description:
        ------------
        Sort the list of folder pairs based on their similarity scores in descending order.
        Args:
        ----- 
            log : Log
                #: The Log containing the duplicate results

        
        Outputs:
        --------
            sorted : list[(Folder, Folder)]
                #: The sorted list of folder pairs

    &#34;&#34;&#34;
    unsorted = [(pair, log.similarPairScoresDict[pair]) for pair in log.similarPairScoresDict]
    #unsorted = [(pair, score) for pair, score in log.similarPairScoresDict.items()]
    if(len(unsorted) == 0):
        return unsorted
    # Create array of scores to sort
    scores = np.array([pairScore[1] for pairScore in unsorted])
    # Sort in descending order
    sortIdx = np.flip(np.argsort(scores))
    sorted = [unsorted[idx] for idx in sortIdx]
    return sorted

    


def FindDuplicateDirectories(log:Log):
    &#34;&#34;&#34;
        Description:
        ------------
        For all the duplicated files, check if they reside in duplicate directories, and then recursively check their parents if found duplicate directories.
        Returns the result and summary lines.
        Args:
        ----- 
            log : Log
                #: The Log containing the duplicate results

        
        Outputs:
        --------
            summaryLines : list[str]
                #: The list of summary lines
            folderResultLines : list[str]
                #: The list of folder pair result lines

    &#34;&#34;&#34;

    for fpLog in log.dupeFpLogList:
        # Compare pairs of parent folders if any duplicate files reside in multiple locations (skips any pairs already compared)
        _basePairs = GetFolderPairs(fpLog)
        if(_basePairs != None):
            # Skip any pairs that have already been calculated
            # Must check dict not empty otherwise list comprehension fails
            if(len(log.similarPairScoresDict) &gt; 0):
                basePairs = [pair for pair in _basePairs if pair not in log.similarPairScoresDict]
            else:
                basePairs = _basePairs
            # Calculate pair -&gt; recursive similarities 
            pairRecursiveSimilaritiesDict = compareAllPairParents(basePairs, log)
            log.allPairRecursiveScores.update(pairRecursiveSimilaritiesDict)
    
    summaryLines, folderResultLines = writeDuplicateDirsResult(log)
    return summaryLines, folderResultLines


# Get all possible pairs of folders that contain the duplicated file
def GetFolderPairs(fpLog:DupeFpLog):
    &#34;&#34;&#34;
        Description:
        ------------
        Given a duplicated file (passed as a DupeFpLog) obtain all possible pairs of folders that contain any of the duplicates.
        Uses itertools.combinations to find all pairwise combinations of unique folders that contain any of the duplicated files.

        
        Args:
        ----- 
            fpLog : DupeFpLog
                #: The DupeFpLog of this duplicated file

        
        Outputs:
        --------
            folderPairs : list[(Folder, Folder)]
                #: Dict containing list of base pair -&gt; list of recursive parent pair scores

    &#34;&#34;&#34;
    allParents = [ParsePath(file).parent for file in fpLog.dupeFiles]
    # Check for unique parent folders
    uniqueParents = list(unique_everseen(allParents))
    # Only compare if parent folders are different
    if(len(uniqueParents)==1):
        return
    foldersToCheck = [PATH_DICT[parent] for parent in uniqueParents]
    # Get all pair combinations 
    _folderPairs = list(itertools.combinations(foldersToCheck, 2))
    # Remove any pairs already checked
    folderPairs = [pair for pair in _folderPairs if pair not in fpLog.log.similarPairScoresDict]
    return folderPairs
    

# Given a number of folder pairs, compare the similarity of their parent directories (recursively)
def compareAllPairParents(basePairsToCheck, log:Log):
    &#34;&#34;&#34;
        Description:
        ------------
        Given a list of similar base pairs, recursively check the parent similarities of each pair using the recursePairParents function.

        
        Args:
        ----- 
            basePairsToCheck : list[(Folder, Folder)]
                #: The list of all starting pairs of folders
            log : Log
                #: The log object with the duplicate file results
        
        Outputs:
        --------
            allPairsRecursiveSimDict : dict[(Folder, Folder), list[((Folder, Folder), int)]]
                #: Dict containing list of base pair -&gt; list of recursive parent pair scores

    &#34;&#34;&#34;
    
    # Score tuples have form ((folder1, folder2), score)
    basePairScores = compareFolderPairs(basePairsToCheck, log)
    # Dict to store basePair -&gt; recursive list of parent pair similarities
    allPairsRecursiveSimDict = dict()
    for baseScore in basePairScores:
        # Recursively check the parent folders of the base pair
        parentScoresList = list()
        parentScoresList = recursePairParents(baseScore[0], parentScoresList, log, threshold=log.config.similarityThreshold)
        # Returns empty list if the parent pair are the same folder
        if(len(parentScoresList)&gt;0):
            # Add the list of parent scores for this base pair
            allPairsRecursiveSimDict[baseScore[0]] = parentScoresList
        #print(&#34;Folder1: &#34; + str(parsePath(score[0][0]).as_posix()) + &#34;\nFolder2: &#34; + str(parsePath(score[0][0]).as_posix()) +&#34;\nSimilarity: &#34; + f&#34;{score[1]*100:2.2f}%&#34;)
    return allPairsRecursiveSimDict
    


# Recursively check the parents of the similar pair - yields parent pair score at each depth of recursion 
def recursePairParents(basePair, parentScoresList, log, threshold=1):
    &#34;&#34;&#34;
        Description:
        ------------
        Given a similar base pair, recursively check the parent folder pair similarities.
        This function is called recursively and appends the next level up of parent scores up to the previous list of parent scores.

        
        Args:
        ----- 
            basePair : (Folder, Folder)
                #: The starting pair of folders
            parentScoresList : list[((Folder, Folder), int)]
                #: The list of already calculated pair scores to append to 
            log : Log
                #: The log object with the duplicate file results
        
        Outputs:
        --------
            parentScoresList : list[((Folder, Folder), int)]
                #: The list of already calculated pair scores with the newly appended parent pair score

    &#34;&#34;&#34;
    
    # Do not consider parents beyond the root
    if(basePair[0].isTree == True or basePair[1].isTree == True):
        # Considering pair beyond root, return calculated scores if any
        return parentScoresList
    else:   
        # Parse path parents into list for easily constructing the tuple
        baseParentPathList = list((ParsePath(basePair[0]).parent, ParsePath(basePair[1]).parent))
        # Parents guaranteed to be in dict, cannot have pair that has parents higher than the tree root  
        parentPair = tuple([ParseFobject(parentPath) for parentPath in baseParentPathList])
    
    # Check if pair have the same parent - return list of already calculated scores (if any)
    if(parentPair[0] == parentPair[1]):
        return parentScoresList
    else:
        # Compare and score the parent folder pair - single pair must be passed as one element list
        # Returns list of tuples - hence the [0])
        parentsScore = compareFolderPairs(list({parentPair}), log) 

        # Ensure compareFolderPairs returned non-empty list
        if(parentsScore):
            # Recursively check parent parents if above threshold
            if(parentsScore[0][1] &gt;= threshold):
                parentScoresList.append(parentsScore[0])
                recursePairParents(parentsScore[0][0], parentScoresList, log, threshold=threshold)
        
    
    return parentScoresList
    
# Compare a list of folder pairs and assign a similarity score to each
def compareFolderPairs(folderPairs:list, log:Log):
    &#34;&#34;&#34;
        Description:
        ------------
        Given a list of folder pairs, calculate their similarity based on the intersection/union of the number of duplicated files.

        
        Args:
        ----- 
            folderPairs : list[(Folder, Folder)]
                #: List of pairs of folders
            log : Log
                #: The log object with the duplicate file results
        
        Outputs:
        --------
            pairScores : list[((Folder, Folder), int)]
                #: List of pairs that contains the folder pair and their respective scores

    &#34;&#34;&#34;
    # List of files for each pair - only add if both lists not empty
    fileListPairs = [(pair[0].files, pair[1].files) for pair in folderPairs if (pair[0].files and pair[1].files)]
    # Fingerprints for each pair list of files
    fingerprintPairs = [([file.fingerprint for file in pair[0] if file != None], [file.fingerprint for file in pair[1] if file != None]) for pair in fileListPairs]
    # Find any unique fingerprint pairs
    uniqueFpPairs = [( list(unique_everseen(fpPair[0])), list(unique_everseen(fpPair[1])) ) for fpPair in fingerprintPairs]

    pairScores = list()
    if(uniqueFpPairs):
        for idx, pair in enumerate(uniqueFpPairs):
            # Common elements
            equal = [fp for fp in pair[0] if fp in pair[1]]
            # Uncommon elements
            unequal = [fp for fp in pair[0] if fp not in pair[1]]
            # Must also add any fp in pair[1] not in pair[0]
            unequal.extend([fp for fp in pair[1] if fp not in pair[0]])
            # Calculate score based on ratio between unequal/total elements
        
            score = 1 - len(unequal)/(len(equal)+len(unequal))
            pairScores.append((folderPairs[idx], score))
            # Add the folder pair and score to dict (stored on master DupeLog object)
            log.similarPairScoresDict[folderPairs[idx]] = score
    
    return pairScores


# ======================================================== #
# ======================== RESULTS ======================= #
# ======================================================== #


def write_duplicate_result_lines(log, iteration):
    &#34;&#34;&#34;
        Description:
        ------------
        Write the results of the file duplicate search. Writes list of duplicated files (from each DupeFpLog) and the duplicated storage space.

        
        Args:
        ----- 
            log : Log
                #: The log object holding the results
        
        Outputs:
        --------
            summaryLines : list[str]
                #: List of summary lines
            resultLines : list[str]
                List of result lines

    &#34;&#34;&#34;
    
    summaryLines = list()
    resultLines = list()

    summaryLines.append(&#34;File duplicate search summary: \n&#34;)
    summaryLines.append(&#34;Scanned &#34; + str(len(log.FpList)) + &#34; files (&#34;+str(size(log.totScannedStorage))+&#34;) &#34; + &#34;in &#34; + f&#34;{round(log.elapsedTime,2):00.00}&#34; +&#34;s / &#34; + f&#34;{round(log.elapsedTime/60,2):00.00}&#34; + &#34;m \n&#34; )
    summaryLines.append(&#34;Threshold: &#34; + size(log.config.sizeThreshold) + &#34; - Ignored &#34; + str(log.filesIgnored) + &#34; files \n&#34;)
    summaryLines.append(&#34;Excluded directories: &#34; + str([s for s in log.config.excludedPaths])+&#34;\n&#34;)
    summaryLines.append(&#34;Found &#34; + str(len(log.dupeFpList)) + &#34; duplicate fingerprints \n&#34; )
    
    if(log.totScannedStorage != 0): # To avoid div/0
        summaryLines.append(&#34;Total duplicated storage: &#34; + size(log.totDupeStorage) + &#34;     (&#34; + str(round(((log.totDupeStorage/log.totScannedStorage)*100), 1)) +&#34;%&#34; + &#34; of total)&#34;)
    

    resultLines.append(f&#34;\n{&#39;-&#39; * 100}\n{&#39; &#39; *45}Files\n{&#39;-&#39; * 100}\n&#34;)
    sortedDupeLogList = OrderDupeStorage(log)
    for dupeFpLog in sortedDupeLogList:
        # Duplicate files corresponding to this fingerprint
        dupeFiles = dupeFpLog.dupeFiles
        filesize, dupedStorage = dupeFpLog.fileSize, dupeFpLog.dupedStorage
        
        resultLines.append(&#34;File: (&#34; + size(filesize) + &#34;) - &#34; + str(len(dupeFiles)) + &#34; duplicates: \n&#34;)
        
        previousFolder = None
        for dupe in dupeFiles:
            if(dupe.path.parent == previousFolder):
                currentFolder = previousFolder
            else:
                currentFolder = dupe.path.parent
            # Write full path if writing a new folder
            if(currentFolder != previousFolder):
                resultLines.append(f&#34;{&#39; &#39;*6}&#34; + str(dupe.path.as_posix()) + &#34;\n&#34;)
            # Abbreviate the parent 
            else:
                resultLines.append(f&#34;{&#39; &#39;*6}&#34; + f&#34;{&#39; &#39;* (len(str(dupe.path.parent))-3)}&#34; +&#34;.../&#34;+ dupe.path.name + &#34;\n&#34;)
            previousFolder = currentFolder
        
        resultLines.append(&#34;\nDuplicated storage space: (&#34; + str(size(dupedStorage))+&#34;)\n&#34;)
        resultLines.append(&#34;Common root: &#34; + str(dupeFpLog.commonRoot.as_posix()) + &#34;\n &#34;)
        resultLines.append(f&#34;\n {&#39;-&#39; * 100}\n\n&#34;)
    return summaryLines, resultLines



# Store the duplicate finding results to a file
def store_duplicate_results(lines, outPath, iteration):
        
    outFile = outPath / (&#34;FileDuplicates_&#34;+str(iteration)+&#34;.txt&#34;)
    
    with open(outFile, &#34;w+&#34;) as writer:
        
        writer.writelines(lines)


def writeDuplicateDirsResult(log:Log):

    &#34;&#34;&#34;
        Description:
        ------------
        Write the results of the folder similarity duplicate search. Writes list of folder pairs + similarity scores and a short summary.

        
        Args:
        ----- 
            log : Log
                #: The log object holding the results
        
        Outputs:
        --------
            summaryLines : list[str]
                #: List of summary lines
            resultLines : list[str]
                List of result lines

    &#34;&#34;&#34;

    summaryLines = list()
    resultLines = list()
    sortedPairScores = SortPairScores(log)

    aboveThreshold = [pairScore for pairScore in sortedPairScores if pairScore[1] &gt;= log.config.similarityThreshold]

    summaryLines.append(&#34;Duplicate directory search results: \n&#34;)
    summaryLines.append(&#34;Scanned &#34; + str(len(sortedPairScores)) + &#34; directory pairs\n&#34; )
    summaryLines.append(&#34;Found &#34; + str(len(aboveThreshold)) + &#34; directory pairs with similarity above the threshold (&#34; + f&#34;{log.config.similarityThreshold*100:2.2f}%)\n&#34;)
    summaryLines.append(&#34;Found &#34; + str(len(log.allPairRecursiveScores)) + &#34; directory pairs with similar parents&#34;)

    resultLines.append(f&#34;\n{&#39;-&#39; * 100}\n{&#39; &#39; *35}Similar Parents\n{&#39;-&#39; * 100}\n&#34;)

    for basePair, similarParents in log.allPairRecursiveScores.items():
        resultLines.append(&#34;From pair: (&#34; + ParsePath(basePair[0]).as_posix() + &#34;, &#34; + ParsePath(basePair[1]).as_posix() +&#34;)&#34;)
        resultLines.append(&#34;Recursive search identified similar parents: &#34;)
        for parentPair in similarParents:
            resultLines.append(&#34;Pair: (&#34; + ParsePath(parentPair[0][0]).as_posix() + &#34;, &#34; + ParsePath(parentPair[0][1]).as_posix() +&#34;)&#34; + &#34; #: &#34; + &#34;Similarity: &#34; + f&#34;{parentPair[1]*100:2.2f}%\n&#34; )


    resultLines.append(f&#34;\n{&#39;-&#39; * 100}\n{&#39; &#39; *38}Folder pairs\n{&#39;-&#39; * 100}\n&#34;)


    resultLines.append(&#34;Pairs above threshold (&#34; + f&#34;{log.config.similarityThreshold*100:2.2f}%): \n&#34;)
    for pair, score in aboveThreshold:
        resultLines.append(&#34;Pair: (&#34; + ParsePath(pair[0]).as_posix() + &#34;, &#34; + ParsePath(pair[1]).as_posix() +&#34;)&#34; + &#34; #: &#34; + &#34;Similarity: &#34; + f&#34;{score*100:2.2f}%\n&#34; )
    resultLines.append(f&#34;\n{&#39;-&#39; * 100}\n&#34;)
    resultLines.append(&#34;Pairs below threshold: \n&#34;)
    for pair, score in sortedPairScores[len(aboveThreshold):]:
        resultLines.append(&#34;Pair: (&#34; + ParsePath(pair[0]).as_posix() + &#34;, &#34; + ParsePath(pair[1]).as_posix() +&#34;)&#34; + &#34; #: &#34; + &#34;Similarity:&#34; + f&#34;{score*100:2.2f}%&#34; )

    return summaryLines, resultLines


# ======================================================== #
# ====================== MAIN SEARCH ===================== #
# ======================================================== #

# The main function that configures and runs the duplicate search
def findDuplicatesMain(config, writeFile=False, cluster=True, wholeTree=True):
    &#34;&#34;&#34;
        Description:
        ------------
        The main function used to search for duplicates, requires a config object,
        writeFile specifies whether the output should generate a file (old version) 
        or simply return a list of lines to display elsewhere (django)

        
        Args:
        -----
            config : Config
                #: Duplicate search configuration object

        Kwargs:
        -------
            writeFile : bool : default=False
                #: Whether to output the results to a file or simply store as list of lines
            cluster : bool : default=True
                #: Whether to apply pre size and extension clustering to reduce number of fingerprinted files
            wholeTree : bool : default=True
                #: If true, only search within the specified included folders from the config/ 
        
        Outputs:
        --------
            fileSummaryLines : list[str] 
                #: The lines for the summary of the duplicate search
            fileResultLines : list[str]
                #: The lines with the list of duplicate file results
            folderResultLines : list[str]
                #: The lines with the similar folder results
    &#34;&#34;&#34;

    # Create the list of all folders to scan
    foldersToScan = list()
    global PATH_DICT
    if(wholeTree):
        # Add every folder in the tree to list to scan
        for tree in config.trees:
            foldersToScan.append(tree.rootFolder)
            foldersToScan.extend(ListSubdirsAtDepth(tree.rootFolder, config.depth))
            # Total list of all folders to scan
            foldersToScan = RemoveExcludedFolders(foldersToScan, config.excludedPaths)

            # Store the list of folders to scan
            config.log.scannedFolders = foldersToScan
    

    if(cluster):
        clusterStartTime = time.time()
        
        if(wholeTree):
            # Form list of all files to be scanned (for size clustering)
            allFiles = ListAllToplevelFiles(foldersToScan) 
        else:
            # Use the included list of all files
            # TODO - Cannot yet exclude folders if using pruned tree
            allFiles = [ParseFobject(path) for path in config.includedPaths]

        # Cluster the list of all files 
        
        fileClusters = ClusterAllFiles(allFiles, config.clusterThreshold)
        # Record total clustering time
        config.log.totClusterTime = time.time() - clusterStartTime

        # Fingerprint all clustered files
        allClusteredFiles = list()
        fpStartTime = time.time()
        for cluster in fileClusters:
            allClusteredFiles.extend(cluster)
        
        FingerprintAllFiles(allClusteredFiles, config.log)
        config.log.totFpTime = time.time() - fpStartTime
    
    else:
        fpStartTime = time.time()
        # Fingerprint all included folders 
        config.log = FingerprintAllFolders(foldersToScan, config.log)
        # Record total time spent fingerprinting
        config.log.totFpTime = time.time() - fpStartTime
    
    

    # Find duplicate fingerprints and wait for append
    running = True
    iteration = -1
    while running:
        iteration +=1
        
        if(iteration &gt; 0): # Clear previous iteration duplicates (to remove appended exclusions)
            config.log.dupeFpLogList.clear()
            config.log.dupeFpList.clear()
        
        # Run the duplicate search
        config.log = find_duplicate_fingerprints(config.log)

        # Search for duplicate directories, starting from duplicate files that reside in multiple directories
        folderSummaryLines, folderResultLines = FindDuplicateDirectories(config.log)

        
        # Write all the lines of the duplicate results
        fileSummaryLines, fileResultLines = write_duplicate_result_lines(config.log, iteration)

        fileSummaryLines.extend(folderSummaryLines)

        # Log the duplicate finding results in a file
        if(writeFile):
            store_duplicate_results(fileResultLines, config.runPath, iteration)
            input(&#34;Waiting for append file...Add desired excluded folders to config/append.txt then press any key: &#34;)
            appendExcludedRoots(config, config.appendFile)
        # Otherwise return the lines to be written elsewhere (eg on webpage)
        else:
            return fileSummaryLines, fileResultLines, folderResultLines
            






# ======================================================== #
# ======================== UNUSED ======================== #
# ======================================================== #

# # Currently unused
# def findSimilarTuples(log:Log, threshold=0.9):
    
#     for pair, score in log.similarPairScoresDict.items():
#         # Take the first folder in the pair, 
#         base = pair[0]
#         # Check if this folder is already in the similarTupleScoresDict
#         check = False
#         for tuple in log.similarTupleScoresDict:
#             if(check == True):
#                 break
#             for folder in tuple:
#                 if(check == True):
#                     break
#                 if(base == folder):
#                     # Folder has already been added
#                     check = True
#                     break
#         # If the folder was already checked, move to next folder
#         if(check):
#             break       
        
#         # Folder was not yet checked
#         # Find all pairs that contain this folder
#         basePairs = [pair for pair in log.similarPairScoresDict if (pair[0]==base or pair[1]==base)]
#         # Check for similar folders above the threshold
#         similarFolders = list()
#         averageSimilarity = 0
#         for basePair in basePairs:
#             score = log.similarPairScoresDict[basePair]
#             if(score &gt; threshold):
#                 # Add the pair folder to list of similar folders 
#                 similarFolders.append(basePair[1])
#                 averageSimilarity += score
#         if(len(similarFolders) &gt; 0):
#             averageSimilarity = averageSimilarity/len(similarFolders)
#             # All folders in list should now be above similarity threshold
#             similarTuple = tuple(similarFolders)
#             log.similarTupleScoresDict[similarTuple] = averageSimilarity
#             print(similarTuple)
#         else:
#             continue




# # Filter the folder pairs based on the score threshold (default 90%)
# def filterSimilarFolders(log:Log, threshold=0.9):
#     log.filteredPairScoresDict = dict()
#     for pair, score in log.similarPairScoresDict.items():
#         if(score &gt; threshold):
#             log.filteredPairScoresDict[pair] = threshold



# # Given a set of duplicate files find their maximal subtree root (furthest common parent from all files)
# def findCommonRoot(files:list):
#     # Number of paths to compare
#     numPaths = len(files)
#     # List all parent paths
#     paths = [PurePath(ParsePath(file).parent) for file in files]
    
#     # First check if all files have the same parent - can then skip individual parts check
#     # Checks if #occurences of the first element is equal to the len - if so then list only contains copies of that element 
#     if(paths.count(paths[0]) == numPaths):
#         return paths[0]

#     # Check individual path parts
    
#     # List all path parts
#     allParts = [path.parts for path in paths]
#     minLength = min([len(parts) for parts in allParts])
    
#     length = 0
#     zipped = zip(*allParts)
#     # Iterate through parts until find uncommon part
#     # Zip list of all parts and compare parts at each level 
#     for idx, parts in enumerate(zip(*allParts)):
#         # Check if all elements in parts list are the same 
#         if(idx &lt; minLength - 1):
#             if(parts.count(parts[0]) == len(parts)):
#                 continue
#             else:
#                 # Parts list is unequal, previous part was common root
#                 length = idx - 1
#         else:
#             # Reached minLength. 
#             length = minLength
    
#     # Construct the common root by adding parts up to the common length
#     commonRoot = &#39;&#39;
#     for i in range(length):
#         commonRoot += allParts[0][i] + &#34;/&#34; 

#     return ParsePath(commonRoot)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="duplicates.CalculateDuplicatedStorage"><code class="name flex">
<span>def <span class="ident">CalculateDuplicatedStorage</span></span>(<span>dupeFiles)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def CalculateDuplicatedStorage(dupeFiles):
    # Need first file only (all duplicates will have the same size)
    fileSize = dupeFiles[0].netSize
    dupedStorage = fileSize * (len(dupeFiles) - 1)
    return fileSize, dupedStorage</code></pre>
</details>
</dd>
<dt id="duplicates.ClusterAllFiles"><code class="name flex">
<span>def <span class="ident">ClusterAllFiles</span></span>(<span>files: list, sizeThreshold)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description</h2>
<p>Cluster the list of files based on the specified threshold. Then further cluster within each cluster based on file extensions.
This reduced the number of fingerprints required to be calculated - since only files that are identical in size and extension could ever be duplicates.
</p>
<h2 id="args">Args:</h2>
<pre><code>files : list[File or pathlike]
    #: The list of files (or pathlike file objects) to cluster
sizeThreshold : int
    #: The size of the maximum difference between sequential elements in each cluster (in bytes)
</code></pre>
<h2 id="outputs">Outputs</h2>
<pre><code>allSizeExtClusters: list[list[File]]
    #: List containing all clusters
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ClusterAllFiles(files:list[File], sizeThreshold):
    &#39;&#39;&#39; 
    Description
    -----------
    Cluster the list of files based on the specified threshold. Then further cluster within each cluster based on file extensions.
    This reduced the number of fingerprints required to be calculated - since only files that are identical in size and extension could ever be duplicates.   
    
    Args:
    -----
        files : list[File or pathlike]
            #: The list of files (or pathlike file objects) to cluster
        sizeThreshold : int
            #: The size of the maximum difference between sequential elements in each cluster (in bytes)


    Outputs
    -------
        allSizeExtClusters: list[list[File]]
            #: List containing all clusters
    &#39;&#39;&#39;
    
    # Cluster all files by size
    fileClusters = ClusterBySize(files, sizeThreshold)
    
    allSizeExtClusters = list()
    for cluster in fileClusters:
        # Further cluster the list of files by extension
        extClusters = ClusterByExtension(cluster)
        for extCluster in extClusters:
            # Append the list of size + extension clustered files 
            allSizeExtClusters.append(extCluster[1])
    
    return allSizeExtClusters</code></pre>
</details>
</dd>
<dt id="duplicates.ClusterByExtension"><code class="name flex">
<span>def <span class="ident">ClusterByExtension</span></span>(<span>files: list)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description</h2>
<p>Further cluster files (after size clustering) based on their extension. Any file with an extension that appears only once is dropped. </p>
<h2 id="args">Args:</h2>
<pre><code>files : list[File]
    #: The list of files to cluster
</code></pre>
<h2 id="outputs">Outputs</h2>
<pre><code>extensionClusters: list[list[File]]
    #: List containing all extension clusters (each cluster is a list of files with that extension)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ClusterByExtension(files:list[File]):
    &#39;&#39;&#39; 
    Description
    -----------
    Further cluster files (after size clustering) based on their extension. Any file with an extension that appears only once is dropped. 
    
    
    Args:
    -----
        files : list[File]
            #: The list of files to cluster


    Outputs
    -------
        extensionClusters: list[list[File]]
            #: List containing all extension clusters (each cluster is a list of files with that extension)
    &#39;&#39;&#39;  
    extensions = [file.extension for file in files]
    # List which extensions are present 
    uniqueExt = unique_everseen(extensions)
    # List to store (ext, cluster)
    extensionClusters = list()
    
    for checkExt in uniqueExt:
        extCluster = list()
        # Add to cluster if extensions match
        for idx, ext in enumerate(extensions):
            if(ext == checkExt):
                extCluster.append(files[idx])
        # Only include clusters with more than one file
        if(len(extCluster) &gt; 1):
            extensionClusters.append((checkExt, extCluster))

    return extensionClusters</code></pre>
</details>
</dd>
<dt id="duplicates.ClusterBySize"><code class="name flex">
<span>def <span class="ident">ClusterBySize</span></span>(<span>files: list, threshold=1000)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description</h2>
<p>Cluster the list of files based on the specified threshold. Files are clustered sequentially - i.e files are added to the cluster if the difference between the next - previous file is
less than the threshold.
</p>
<h2 id="args">Args:</h2>
<pre><code>files : list[File or pathlike]
    #: The list of files (or pathlike file objects) to cluster
threshold : int
    #: The size of the maximum difference between sequential elements in each cluster (in bytes)
</code></pre>
<h2 id="outputs">Outputs</h2>
<pre><code>fileClusters: list[list[File]]
    #: List containing all size clusters
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ClusterBySize(files:list, threshold=1000):
    &#39;&#39;&#39; 
    Description
    -----------
    Cluster the list of files based on the specified threshold. Files are clustered sequentially - i.e files are added to the cluster if the difference between the next - previous file is 
    less than the threshold.   
    
    Args:
    -----
        files : list[File or pathlike]
            #: The list of files (or pathlike file objects) to cluster
        threshold : int
            #: The size of the maximum difference between sequential elements in each cluster (in bytes)


    Outputs
    -------
        fileClusters: list[list[File]]
            #: List containing all size clusters 
    &#39;&#39;&#39;
    # Ensure list contains fobjects only
    files = [ParseFobject(file) for file in files]
    # Sort the list of files in size order (descending - using the sortbySizeDesc method)
    sortedFiles = sortbySizeDesc(files)
    sizes = [file.netSize for file in sortedFiles]
    # Cluster the sorted list of files - generator yields (cluster, corresponding indexes from sizes list)
    clusters = list(SizeClusterGen(sizes, threshold))

    # Create a list containing each set of clustered files 
    fileClusters = list()
    for cluster in clusters:
        # Grab the files from the list using the clustered indices
        fileCluster = [files[idx] for idx in cluster[1]]
        fileClusters.append(fileCluster)
    
    return fileClusters</code></pre>
</details>
</dd>
<dt id="duplicates.FindDuplicateDirectories"><code class="name flex">
<span>def <span class="ident">FindDuplicateDirectories</span></span>(<span>log: src.fileorganiser.general.Log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description:</h2>
<p>For all the duplicated files, check if they reside in duplicate directories, and then recursively check their parents if found duplicate directories.
Returns the result and summary lines.
Args:</p>
<hr>
<pre><code>log : Log
    #: The Log containing the duplicate results
</code></pre>
<h2 id="outputs">Outputs:</h2>
<pre><code>summaryLines : list[str]
    #: The list of summary lines
folderResultLines : list[str]
    #: The list of folder pair result lines
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def FindDuplicateDirectories(log:Log):
    &#34;&#34;&#34;
        Description:
        ------------
        For all the duplicated files, check if they reside in duplicate directories, and then recursively check their parents if found duplicate directories.
        Returns the result and summary lines.
        Args:
        ----- 
            log : Log
                #: The Log containing the duplicate results

        
        Outputs:
        --------
            summaryLines : list[str]
                #: The list of summary lines
            folderResultLines : list[str]
                #: The list of folder pair result lines

    &#34;&#34;&#34;

    for fpLog in log.dupeFpLogList:
        # Compare pairs of parent folders if any duplicate files reside in multiple locations (skips any pairs already compared)
        _basePairs = GetFolderPairs(fpLog)
        if(_basePairs != None):
            # Skip any pairs that have already been calculated
            # Must check dict not empty otherwise list comprehension fails
            if(len(log.similarPairScoresDict) &gt; 0):
                basePairs = [pair for pair in _basePairs if pair not in log.similarPairScoresDict]
            else:
                basePairs = _basePairs
            # Calculate pair -&gt; recursive similarities 
            pairRecursiveSimilaritiesDict = compareAllPairParents(basePairs, log)
            log.allPairRecursiveScores.update(pairRecursiveSimilaritiesDict)
    
    summaryLines, folderResultLines = writeDuplicateDirsResult(log)
    return summaryLines, folderResultLines</code></pre>
</details>
</dd>
<dt id="duplicates.FingerprintAllFiles"><code class="name flex">
<span>def <span class="ident">FingerprintAllFiles</span></span>(<span>files: list, log: src.fileorganiser.general.Log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description</h2>
<p>Given a list of files calculate the fingerprint (MD5 hash) for each file. Fingerprints are stored on each file object
and within the log.FpFilesDict which stores the fingerprint -&gt; files with that fingerprint.</p>
<h2 id="args">Args:</h2>
<pre><code>files : list[File]
    #: The list of all files to be fingerprinted
log : Log
    #: The log object for the duplicate search
</code></pre>
<h2 id="outputs">Outputs</h2>
<p>None
#: Results are stored in the log object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def FingerprintAllFiles(files:list[File], log:Log):

    &#39;&#39;&#39; 
    Description
    -----------
    Given a list of files calculate the fingerprint (MD5 hash) for each file. Fingerprints are stored on each file object
    and within the log.FpFilesDict which stores the fingerprint -&gt; files with that fingerprint.
    
    
    Args:
    -----
        files : list[File]
            #: The list of all files to be fingerprinted
        log : Log
            #: The log object for the duplicate search

    Outputs
    -------
    None
        #: Results are stored in the log object
    &#39;&#39;&#39;  

    fpList = list()
    for idx, file in enumerate(files):
        # Ensure passed a fobject
        file = ParseFobject(file)
        # Fingerprint the file
        fp = fingerprintFile(file, log)
        if fp != None:
            fpString = fp.hexdigest()
            file.fingerprint = fpString
            fpList.append(fpString)
            # Add to fingerprint -&gt; files dictionary (check if already added same fingerprint)
            if(fpString in log.FpFilesDict):
                # Dict already contains this fingerprint, append file to file list
                log.FpFilesDict[fpString].append(file)
            else:
                # Add fingerprint to dict (key value must be a list of files - for duplicate fingerprints)
                log.FpFilesDict[fpString] = list({file})
            # Record size in total size scanned
            log.totScannedStorage += file.path.stat().st_size
        # Show elapsed time in console     
        log.elapsedTime = time.time()-log.startTime
        print(&#34;Remaining: &#34; + str(len(files) - idx) + &#34; Elapsed: &#34; + f&#34;{round(log.elapsedTime,1)}&#34; +&#34;s / &#34; + f&#34;{round(log.elapsedTime/60,1)}&#34; + &#34;m &#34; + &#34;  ----  Fingerprinting file: &#34; + str(file.path.as_posix()))
    # Add all fingerprints to log
    log.FpList.extend(fpList)</code></pre>
</details>
</dd>
<dt id="duplicates.FingerprintAllFolders"><code class="name flex">
<span>def <span class="ident">FingerprintAllFolders</span></span>(<span>folders: list, log: src.fileorganiser.general.Log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description</h2>
<p>Fingerprint all files within the given list of folders (the toplevel of each folder only)</p>
<h2 id="args">Args:</h2>
<pre><code>folders : list[Folder]
    #: The list of all folders (subdirectories included as separate folders in the list) to fingerprint files within
log : Log
    #: The log object for the duplicate search
</code></pre>
<h2 id="outputs">Outputs</h2>
<pre><code>log: log
    #: Results are stored in the log object
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def FingerprintAllFolders(folders:list[Folder], log:Log):
    &#39;&#39;&#39; 
    Description
    -----------
    Fingerprint all files within the given list of folders (the toplevel of each folder only)
    
    
    Args:
    -----
        folders : list[Folder]
            #: The list of all folders (subdirectories included as separate folders in the list) to fingerprint files within
        log : Log
            #: The log object for the duplicate search

    Outputs
    -------
        log: log
            #: Results are stored in the log object
    &#39;&#39;&#39;  

    print(&#34;Fingerprinting all folders: &#34;)
    
    # Scan each folder
    for idx, folder in enumerate(folders):
        # Fingerprint each file in the folder
        folderFpList = fingerprintFolder(folder, log)
        log.FpList.extend(folderFpList)
        log.elapsedTime = time.time()-log.startTime
        print(&#34;Remaining: &#34; + str(len(folders) - idx) + &#34; Elapsed: &#34; + f&#34;{round(log.elapsedTime,1)}&#34; +&#34;s = &#34; + f&#34;{round(log.elapsedTime/60,1)}&#34; + &#34;m &#34; + &#34;  ----  Fingerprinting folder: &#34; + folder.path.as_posix())
        # Record storage size 
        for file in folder.files:
            log.totScannedStorage += file.path.stat().st_size
    return log</code></pre>
</details>
</dd>
<dt id="duplicates.GetFolderPairs"><code class="name flex">
<span>def <span class="ident">GetFolderPairs</span></span>(<span>fpLog: <a title="duplicates.DupeFpLog" href="#duplicates.DupeFpLog">DupeFpLog</a>)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description:</h2>
<p>Given a duplicated file (passed as a DupeFpLog) obtain all possible pairs of folders that contain any of the duplicates.
Uses itertools.combinations to find all pairwise combinations of unique folders that contain any of the duplicated files.</p>
<h2 id="args">Args:</h2>
<pre><code>fpLog : DupeFpLog
    #: The DupeFpLog of this duplicated file
</code></pre>
<h2 id="outputs">Outputs:</h2>
<pre><code>folderPairs : list[(Folder, Folder)]
    #: Dict containing list of base pair -&gt; list of recursive parent pair scores
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def GetFolderPairs(fpLog:DupeFpLog):
    &#34;&#34;&#34;
        Description:
        ------------
        Given a duplicated file (passed as a DupeFpLog) obtain all possible pairs of folders that contain any of the duplicates.
        Uses itertools.combinations to find all pairwise combinations of unique folders that contain any of the duplicated files.

        
        Args:
        ----- 
            fpLog : DupeFpLog
                #: The DupeFpLog of this duplicated file

        
        Outputs:
        --------
            folderPairs : list[(Folder, Folder)]
                #: Dict containing list of base pair -&gt; list of recursive parent pair scores

    &#34;&#34;&#34;
    allParents = [ParsePath(file).parent for file in fpLog.dupeFiles]
    # Check for unique parent folders
    uniqueParents = list(unique_everseen(allParents))
    # Only compare if parent folders are different
    if(len(uniqueParents)==1):
        return
    foldersToCheck = [PATH_DICT[parent] for parent in uniqueParents]
    # Get all pair combinations 
    _folderPairs = list(itertools.combinations(foldersToCheck, 2))
    # Remove any pairs already checked
    folderPairs = [pair for pair in _folderPairs if pair not in fpLog.log.similarPairScoresDict]
    return folderPairs</code></pre>
</details>
</dd>
<dt id="duplicates.OrderDupeStorage"><code class="name flex">
<span>def <span class="ident">OrderDupeStorage</span></span>(<span>log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description:</h2>
<p>Sorts the DupeFpLogs based on the duplicated storage size
Args:</p>
<hr>
<pre><code>log : Log
    #: The Log containing the duplicate results
</code></pre>
<h2 id="outputs">Outputs:</h2>
<pre><code>sorted : list[DupeFpLog]
    #: The sorted list of duplicate logs
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def OrderDupeStorage(log):
    &#34;&#34;&#34;
        Description:
        ------------
        Sorts the DupeFpLogs based on the duplicated storage size
        Args:
        ----- 
            log : Log
                #: The Log containing the duplicate results

        
        Outputs:
        --------
            sorted : list[DupeFpLog]
                #: The sorted list of duplicate logs

    &#34;&#34;&#34;
    
    dupeSizesUnsorted = []
    sortedDupeLogList = list()
    
    # Read the duplicated storage amounts for each dupeLog
    for dupeFpLog in log.dupeFpLogList:
        dupeSizesUnsorted.extend({dupeFpLog.dupedStorage})
    
    # Flip the list to sort in descending order
    sortedSizeIndices = np.flip((np.argsort(dupeSizesUnsorted)))
    
    # Populate the sorted list of paths
    for unsortIdx in sortedSizeIndices:
        sortedDupeLogList.extend({log.dupeFpLogList[unsortIdx]})
    
    return sortedDupeLogList</code></pre>
</details>
</dd>
<dt id="duplicates.SizeClusterGen"><code class="name flex">
<span>def <span class="ident">SizeClusterGen</span></span>(<span>iterable, threshold)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description</h2>
<p>Generator that yields the clusters of files (based on size) within the specified threshold. Adapted from <a href="https://stackoverflow.com/a/15801233">https://stackoverflow.com/a/15801233</a>
Iteratively adds the next element in the list of sizes if the difference between next - previous is less than the threshold.
Possible that this can generate large clusters if all sequential files are within the threshold - i.e 1000, 1999, 2998, 3997, 4997 &hellip; would all fall within the same cluster with threshold 1000.
In practice, a threshold of 1000bytes is sufficient to avoid this possibility with real file sizes ranging in the kilo to megabyte range.
</p>
<h2 id="args">Args:</h2>
<pre><code>iterable : list[int]
    #: The list of file sizes to cluster
threshold : int
    #: The size of the maximum difference between sequential elements in each cluster (in bytes)
</code></pre>
<h2 id="outputs">Outputs</h2>
<pre><code>cluster : list[int]
    #: List of all sizes within the cluster
clusterIdx : int
    #: The index in the original unclustered list of files that correspond to each clustered size (for matching size to file)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def SizeClusterGen(iterable, threshold):
    &#39;&#39;&#39; 
    Description
    -----------
    Generator that yields the clusters of files (based on size) within the specified threshold. Adapted from https://stackoverflow.com/a/15801233 
    Iteratively adds the next element in the list of sizes if the difference between next - previous is less than the threshold.
    Possible that this can generate large clusters if all sequential files are within the threshold - i.e 1000, 1999, 2998, 3997, 4997 ... would all fall within the same cluster with threshold 1000.
    In practice, a threshold of 1000bytes is sufficient to avoid this possibility with real file sizes ranging in the kilo to megabyte range.  
    
    Args:
    -----
        iterable : list[int]
            #: The list of file sizes to cluster
        threshold : int
            #: The size of the maximum difference between sequential elements in each cluster (in bytes)


    Outputs
    -------
        cluster : list[int]
            #: List of all sizes within the cluster
        clusterIdx : int
            #: The index in the original unclustered list of files that correspond to each clustered size (for matching size to file) 
    &#39;&#39;&#39;
    
    # Initialise at inf so the first difference calculated is always ignored 
    prev = float(&#34;inf&#34;)
    # The clustered set of sizes
    cluster = []
    # The index in the original list of each item in the cluster
    clusterIdx = []

    for idx, item in enumerate(iterable):
        # Check the absolute difference between the previous item (in desc order so diff always &lt;= 0)
        if np.absolute(item - prev) &lt;= threshold:
            
            # Record the item and original list index if difference below threshold
            cluster.append(item)
            clusterIdx.append(idx)
            
        else:
            if len(cluster) &gt; 1 and len(clusterIdx) &gt; 1:
                yield cluster, clusterIdx
            # Create a new cluster starting with this item
            cluster = [item]
            clusterIdx = [idx]
            
        # Move to the next item
        prev = item
    # Yield the current cluster if reached end of iterable
    if len(cluster) &gt; 1 and len(clusterIdx) &gt; 1:
        yield cluster, clusterIdx</code></pre>
</details>
</dd>
<dt id="duplicates.SortPairScores"><code class="name flex">
<span>def <span class="ident">SortPairScores</span></span>(<span>log: src.fileorganiser.general.Log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description:</h2>
<p>Sort the list of folder pairs based on their similarity scores in descending order.
Args:</p>
<hr>
<pre><code>log : Log
    #: The Log containing the duplicate results
</code></pre>
<h2 id="outputs">Outputs:</h2>
<pre><code>sorted : list[(Folder, Folder)]
    #: The sorted list of folder pairs
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def SortPairScores(log:Log):
    &#34;&#34;&#34;
        Description:
        ------------
        Sort the list of folder pairs based on their similarity scores in descending order.
        Args:
        ----- 
            log : Log
                #: The Log containing the duplicate results

        
        Outputs:
        --------
            sorted : list[(Folder, Folder)]
                #: The sorted list of folder pairs

    &#34;&#34;&#34;
    unsorted = [(pair, log.similarPairScoresDict[pair]) for pair in log.similarPairScoresDict]
    #unsorted = [(pair, score) for pair, score in log.similarPairScoresDict.items()]
    if(len(unsorted) == 0):
        return unsorted
    # Create array of scores to sort
    scores = np.array([pairScore[1] for pairScore in unsorted])
    # Sort in descending order
    sortIdx = np.flip(np.argsort(scores))
    sorted = [unsorted[idx] for idx in sortIdx]
    return sorted</code></pre>
</details>
</dd>
<dt id="duplicates.compareAllPairParents"><code class="name flex">
<span>def <span class="ident">compareAllPairParents</span></span>(<span>basePairsToCheck, log: src.fileorganiser.general.Log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description:</h2>
<p>Given a list of similar base pairs, recursively check the parent similarities of each pair using the recursePairParents function.</p>
<h2 id="args">Args:</h2>
<pre><code>basePairsToCheck : list[(Folder, Folder)]
    #: The list of all starting pairs of folders
log : Log
    #: The log object with the duplicate file results
</code></pre>
<h2 id="outputs">Outputs:</h2>
<pre><code>allPairsRecursiveSimDict : dict[(Folder, Folder), list[((Folder, Folder), int)]]
    #: Dict containing list of base pair -&gt; list of recursive parent pair scores
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compareAllPairParents(basePairsToCheck, log:Log):
    &#34;&#34;&#34;
        Description:
        ------------
        Given a list of similar base pairs, recursively check the parent similarities of each pair using the recursePairParents function.

        
        Args:
        ----- 
            basePairsToCheck : list[(Folder, Folder)]
                #: The list of all starting pairs of folders
            log : Log
                #: The log object with the duplicate file results
        
        Outputs:
        --------
            allPairsRecursiveSimDict : dict[(Folder, Folder), list[((Folder, Folder), int)]]
                #: Dict containing list of base pair -&gt; list of recursive parent pair scores

    &#34;&#34;&#34;
    
    # Score tuples have form ((folder1, folder2), score)
    basePairScores = compareFolderPairs(basePairsToCheck, log)
    # Dict to store basePair -&gt; recursive list of parent pair similarities
    allPairsRecursiveSimDict = dict()
    for baseScore in basePairScores:
        # Recursively check the parent folders of the base pair
        parentScoresList = list()
        parentScoresList = recursePairParents(baseScore[0], parentScoresList, log, threshold=log.config.similarityThreshold)
        # Returns empty list if the parent pair are the same folder
        if(len(parentScoresList)&gt;0):
            # Add the list of parent scores for this base pair
            allPairsRecursiveSimDict[baseScore[0]] = parentScoresList
        #print(&#34;Folder1: &#34; + str(parsePath(score[0][0]).as_posix()) + &#34;\nFolder2: &#34; + str(parsePath(score[0][0]).as_posix()) +&#34;\nSimilarity: &#34; + f&#34;{score[1]*100:2.2f}%&#34;)
    return allPairsRecursiveSimDict</code></pre>
</details>
</dd>
<dt id="duplicates.compareFolderPairs"><code class="name flex">
<span>def <span class="ident">compareFolderPairs</span></span>(<span>folderPairs: list, log: src.fileorganiser.general.Log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description:</h2>
<p>Given a list of folder pairs, calculate their similarity based on the intersection/union of the number of duplicated files.</p>
<h2 id="args">Args:</h2>
<pre><code>folderPairs : list[(Folder, Folder)]
    #: List of pairs of folders
log : Log
    #: The log object with the duplicate file results
</code></pre>
<h2 id="outputs">Outputs:</h2>
<pre><code>pairScores : list[((Folder, Folder), int)]
    #: List of pairs that contains the folder pair and their respective scores
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compareFolderPairs(folderPairs:list, log:Log):
    &#34;&#34;&#34;
        Description:
        ------------
        Given a list of folder pairs, calculate their similarity based on the intersection/union of the number of duplicated files.

        
        Args:
        ----- 
            folderPairs : list[(Folder, Folder)]
                #: List of pairs of folders
            log : Log
                #: The log object with the duplicate file results
        
        Outputs:
        --------
            pairScores : list[((Folder, Folder), int)]
                #: List of pairs that contains the folder pair and their respective scores

    &#34;&#34;&#34;
    # List of files for each pair - only add if both lists not empty
    fileListPairs = [(pair[0].files, pair[1].files) for pair in folderPairs if (pair[0].files and pair[1].files)]
    # Fingerprints for each pair list of files
    fingerprintPairs = [([file.fingerprint for file in pair[0] if file != None], [file.fingerprint for file in pair[1] if file != None]) for pair in fileListPairs]
    # Find any unique fingerprint pairs
    uniqueFpPairs = [( list(unique_everseen(fpPair[0])), list(unique_everseen(fpPair[1])) ) for fpPair in fingerprintPairs]

    pairScores = list()
    if(uniqueFpPairs):
        for idx, pair in enumerate(uniqueFpPairs):
            # Common elements
            equal = [fp for fp in pair[0] if fp in pair[1]]
            # Uncommon elements
            unequal = [fp for fp in pair[0] if fp not in pair[1]]
            # Must also add any fp in pair[1] not in pair[0]
            unequal.extend([fp for fp in pair[1] if fp not in pair[0]])
            # Calculate score based on ratio between unequal/total elements
        
            score = 1 - len(unequal)/(len(equal)+len(unequal))
            pairScores.append((folderPairs[idx], score))
            # Add the folder pair and score to dict (stored on master DupeLog object)
            log.similarPairScoresDict[folderPairs[idx]] = score
    
    return pairScores</code></pre>
</details>
</dd>
<dt id="duplicates.findDuplicatesMain"><code class="name flex">
<span>def <span class="ident">findDuplicatesMain</span></span>(<span>config, writeFile=False, cluster=True, wholeTree=True)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description:</h2>
<p>The main function used to search for duplicates, requires a config object,
writeFile specifies whether the output should generate a file (old version)
or simply return a list of lines to display elsewhere (django)</p>
<h2 id="args">Args:</h2>
<pre><code>config : Config
    #: Duplicate search configuration object
</code></pre>
<h2 id="kwargs">Kwargs:</h2>
<pre><code>writeFile : bool : default=False
    #: Whether to output the results to a file or simply store as list of lines
cluster : bool : default=True
    #: Whether to apply pre size and extension clustering to reduce number of fingerprinted files
wholeTree : bool : default=True
    #: If true, only search within the specified included folders from the config/
</code></pre>
<h2 id="outputs">Outputs:</h2>
<pre><code>fileSummaryLines : list[str] 
    #: The lines for the summary of the duplicate search
fileResultLines : list[str]
    #: The lines with the list of duplicate file results
folderResultLines : list[str]
    #: The lines with the similar folder results
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def findDuplicatesMain(config, writeFile=False, cluster=True, wholeTree=True):
    &#34;&#34;&#34;
        Description:
        ------------
        The main function used to search for duplicates, requires a config object,
        writeFile specifies whether the output should generate a file (old version) 
        or simply return a list of lines to display elsewhere (django)

        
        Args:
        -----
            config : Config
                #: Duplicate search configuration object

        Kwargs:
        -------
            writeFile : bool : default=False
                #: Whether to output the results to a file or simply store as list of lines
            cluster : bool : default=True
                #: Whether to apply pre size and extension clustering to reduce number of fingerprinted files
            wholeTree : bool : default=True
                #: If true, only search within the specified included folders from the config/ 
        
        Outputs:
        --------
            fileSummaryLines : list[str] 
                #: The lines for the summary of the duplicate search
            fileResultLines : list[str]
                #: The lines with the list of duplicate file results
            folderResultLines : list[str]
                #: The lines with the similar folder results
    &#34;&#34;&#34;

    # Create the list of all folders to scan
    foldersToScan = list()
    global PATH_DICT
    if(wholeTree):
        # Add every folder in the tree to list to scan
        for tree in config.trees:
            foldersToScan.append(tree.rootFolder)
            foldersToScan.extend(ListSubdirsAtDepth(tree.rootFolder, config.depth))
            # Total list of all folders to scan
            foldersToScan = RemoveExcludedFolders(foldersToScan, config.excludedPaths)

            # Store the list of folders to scan
            config.log.scannedFolders = foldersToScan
    

    if(cluster):
        clusterStartTime = time.time()
        
        if(wholeTree):
            # Form list of all files to be scanned (for size clustering)
            allFiles = ListAllToplevelFiles(foldersToScan) 
        else:
            # Use the included list of all files
            # TODO - Cannot yet exclude folders if using pruned tree
            allFiles = [ParseFobject(path) for path in config.includedPaths]

        # Cluster the list of all files 
        
        fileClusters = ClusterAllFiles(allFiles, config.clusterThreshold)
        # Record total clustering time
        config.log.totClusterTime = time.time() - clusterStartTime

        # Fingerprint all clustered files
        allClusteredFiles = list()
        fpStartTime = time.time()
        for cluster in fileClusters:
            allClusteredFiles.extend(cluster)
        
        FingerprintAllFiles(allClusteredFiles, config.log)
        config.log.totFpTime = time.time() - fpStartTime
    
    else:
        fpStartTime = time.time()
        # Fingerprint all included folders 
        config.log = FingerprintAllFolders(foldersToScan, config.log)
        # Record total time spent fingerprinting
        config.log.totFpTime = time.time() - fpStartTime
    
    

    # Find duplicate fingerprints and wait for append
    running = True
    iteration = -1
    while running:
        iteration +=1
        
        if(iteration &gt; 0): # Clear previous iteration duplicates (to remove appended exclusions)
            config.log.dupeFpLogList.clear()
            config.log.dupeFpList.clear()
        
        # Run the duplicate search
        config.log = find_duplicate_fingerprints(config.log)

        # Search for duplicate directories, starting from duplicate files that reside in multiple directories
        folderSummaryLines, folderResultLines = FindDuplicateDirectories(config.log)

        
        # Write all the lines of the duplicate results
        fileSummaryLines, fileResultLines = write_duplicate_result_lines(config.log, iteration)

        fileSummaryLines.extend(folderSummaryLines)

        # Log the duplicate finding results in a file
        if(writeFile):
            store_duplicate_results(fileResultLines, config.runPath, iteration)
            input(&#34;Waiting for append file...Add desired excluded folders to config/append.txt then press any key: &#34;)
            appendExcludedRoots(config, config.appendFile)
        # Otherwise return the lines to be written elsewhere (eg on webpage)
        else:
            return fileSummaryLines, fileResultLines, folderResultLines</code></pre>
</details>
</dd>
<dt id="duplicates.find_duplicate_fingerprints"><code class="name flex">
<span>def <span class="ident">find_duplicate_fingerprints</span></span>(<span>log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description</h2>
<p>Given a list of all fingerprints this identifies any that appear more than once. A DupeFpLog instance is
constructed for any duplicated fingerprints, using the fingerprintFiles dict to map
the duplicated fingerprint to the corresponding files </p>
<h2 id="args">Args:</h2>
<pre><code>log : Log
    #: The log object for this search run
</code></pre>
<h2 id="outputs">Outputs</h2>
<pre><code>log : Log
    #: The log with populated results
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_duplicate_fingerprints(log):
    &#39;&#39;&#39; 
    Description
    -----------
    Given a list of all fingerprints this identifies any that appear more than once. A DupeFpLog instance is 
    constructed for any duplicated fingerprints, using the fingerprintFiles dict to map 
    the duplicated fingerprint to the corresponding files 
    
    
    Args:
    -----
        log : Log
            #: The log object for this search run

    Outputs
    -------
    
        log : Log
            #: The log with populated results
    &#39;&#39;&#39;  
    # Find duplicated fingerprints
    log.dupeFpList = list(unique_everseen(duplicates(log.FpList)))

    # Create duplicate logs for this fingerprint (constructs folder pair scores if appropriate)
    for dupeFp in log.dupeFpList:
            FPlog = DupeFpLog(dupeFp, log)
            log.dupeFpLogList.append(FPlog)
            log.totDupeStorage += FPlog.dupedStorage
    
    
    
    log.elapsedTime = time.time()-log.startTime
    return log</code></pre>
</details>
</dd>
<dt id="duplicates.fingerprintFile"><code class="name flex">
<span>def <span class="ident">fingerprintFile</span></span>(<span>file, log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description</h2>
<p>Fingerprint an individual file by calculating its 128-bit MD5 hash (represented as a 32-byte hex string). Fingerprints are stored on each file object
and within the log.FpFilesDict which stores the fingerprint -&gt; files with that fingerprint.</p>
<h2 id="args">Args:</h2>
<pre><code>file : File
    #: The file to fingerprint
log : Log
    #: The log object for the duplicate search
</code></pre>
<h2 id="outputs">Outputs</h2>
<pre><code>fileHash: str
    #: 32-byte hex string of the file fingerprint
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fingerprintFile(file, log):
    &#39;&#39;&#39; 
    Description
    -----------
    Fingerprint an individual file by calculating its 128-bit MD5 hash (represented as a 32-byte hex string). Fingerprints are stored on each file object
    and within the log.FpFilesDict which stores the fingerprint -&gt; files with that fingerprint.
    
    
    Args:
    -----
        file : File
            #: The file to fingerprint
        log : Log
            #: The log object for the duplicate search

    Outputs
    -------
        fileHash: str
            #: 32-byte hex string of the file fingerprint
    &#39;&#39;&#39;  
    
    # Chunksize used in MD5 hash
    chunkSize = 8192
    
    try: 
        # Check if file below threshold size
        if(file.netSize &lt; log.config.sizeThreshold):
            #print(&#34;File below threshold, ignoring&#34;)
            log.filesIgnored += 1
            return None

        with open(ParsePath(file), &#34;rb&#34;) as f:
            fileHash = hashlib.md5()
            while chunk := f.read(chunkSize):
                fileHash.update(chunk)
    except PermissionError:
        print(&#34;Permission error on file: &#34; + str(file.path))
        return None
    except:
        print(&#34;Exception encountered - skipping file: &#34; + str(file.path))
        return None
    
    return fileHash</code></pre>
</details>
</dd>
<dt id="duplicates.fingerprintFolder"><code class="name flex">
<span>def <span class="ident">fingerprintFolder</span></span>(<span>folder, log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description</h2>
<p>Fingerprint all files within the toplevel of the given folder. Fingerprints are stored on each file object
and within the log.FpFilesDict which stores the fingerprint -&gt; files with that fingerprint.</p>
<h2 id="args">Args:</h2>
<pre><code>folder : Folder
    #: The folder to fingerprint files within
log : Log
    #: The log object for the duplicate search
</code></pre>
<h2 id="outputs">Outputs</h2>
<pre><code>fpList: list[str]
    #: List of fingerprint strings within this folder
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fingerprintFolder(folder, log):
    &#39;&#39;&#39; 
    Description
    -----------
    Fingerprint all files within the toplevel of the given folder. Fingerprints are stored on each file object
    and within the log.FpFilesDict which stores the fingerprint -&gt; files with that fingerprint.
    
    
    Args:
    -----
        folder : Folder
            #: The folder to fingerprint files within
        log : Log
            #: The log object for the duplicate search

    Outputs
    -------
        fpList: list[str]
            #: List of fingerprint strings within this folder
    &#39;&#39;&#39;  

    #print(&#34;Fingerprinting folder: &#34; + folder.path.name)
    
    fpList = list()
    #log_storage(folder)
    for f in folder.files:
        fp = fingerprintFile(f, log)
        if fp != None:
            fpString = fp.hexdigest()
            f.fingerprint = fpString
            fpList.append(fpString)
            # Add to fingerprint -&gt; files dictionary (check if already added same fingerprint)
            if(fpString in log.FpFilesDict):
                # Dict already contains this fingerprint, append file to file list
                log.FpFilesDict[fpString].extend({f})
            else:
                # Add fingerprint to dict (key value must be a list of files - for duplicate fingerprints)
                log.FpFilesDict[fpString] = list({f})
        
    return fpList</code></pre>
</details>
</dd>
<dt id="duplicates.recursePairParents"><code class="name flex">
<span>def <span class="ident">recursePairParents</span></span>(<span>basePair, parentScoresList, log, threshold=1)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description:</h2>
<p>Given a similar base pair, recursively check the parent folder pair similarities.
This function is called recursively and appends the next level up of parent scores up to the previous list of parent scores.</p>
<h2 id="args">Args:</h2>
<pre><code>basePair : (Folder, Folder)
    #: The starting pair of folders
parentScoresList : list[((Folder, Folder), int)]
    #: The list of already calculated pair scores to append to 
log : Log
    #: The log object with the duplicate file results
</code></pre>
<h2 id="outputs">Outputs:</h2>
<pre><code>parentScoresList : list[((Folder, Folder), int)]
    #: The list of already calculated pair scores with the newly appended parent pair score
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def recursePairParents(basePair, parentScoresList, log, threshold=1):
    &#34;&#34;&#34;
        Description:
        ------------
        Given a similar base pair, recursively check the parent folder pair similarities.
        This function is called recursively and appends the next level up of parent scores up to the previous list of parent scores.

        
        Args:
        ----- 
            basePair : (Folder, Folder)
                #: The starting pair of folders
            parentScoresList : list[((Folder, Folder), int)]
                #: The list of already calculated pair scores to append to 
            log : Log
                #: The log object with the duplicate file results
        
        Outputs:
        --------
            parentScoresList : list[((Folder, Folder), int)]
                #: The list of already calculated pair scores with the newly appended parent pair score

    &#34;&#34;&#34;
    
    # Do not consider parents beyond the root
    if(basePair[0].isTree == True or basePair[1].isTree == True):
        # Considering pair beyond root, return calculated scores if any
        return parentScoresList
    else:   
        # Parse path parents into list for easily constructing the tuple
        baseParentPathList = list((ParsePath(basePair[0]).parent, ParsePath(basePair[1]).parent))
        # Parents guaranteed to be in dict, cannot have pair that has parents higher than the tree root  
        parentPair = tuple([ParseFobject(parentPath) for parentPath in baseParentPathList])
    
    # Check if pair have the same parent - return list of already calculated scores (if any)
    if(parentPair[0] == parentPair[1]):
        return parentScoresList
    else:
        # Compare and score the parent folder pair - single pair must be passed as one element list
        # Returns list of tuples - hence the [0])
        parentsScore = compareFolderPairs(list({parentPair}), log) 

        # Ensure compareFolderPairs returned non-empty list
        if(parentsScore):
            # Recursively check parent parents if above threshold
            if(parentsScore[0][1] &gt;= threshold):
                parentScoresList.append(parentsScore[0])
                recursePairParents(parentsScore[0][0], parentScoresList, log, threshold=threshold)
        
    
    return parentScoresList</code></pre>
</details>
</dd>
<dt id="duplicates.store_duplicate_results"><code class="name flex">
<span>def <span class="ident">store_duplicate_results</span></span>(<span>lines, outPath, iteration)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def store_duplicate_results(lines, outPath, iteration):
        
    outFile = outPath / (&#34;FileDuplicates_&#34;+str(iteration)+&#34;.txt&#34;)
    
    with open(outFile, &#34;w+&#34;) as writer:
        
        writer.writelines(lines)</code></pre>
</details>
</dd>
<dt id="duplicates.writeDuplicateDirsResult"><code class="name flex">
<span>def <span class="ident">writeDuplicateDirsResult</span></span>(<span>log: src.fileorganiser.general.Log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description:</h2>
<p>Write the results of the folder similarity duplicate search. Writes list of folder pairs + similarity scores and a short summary.</p>
<h2 id="args">Args:</h2>
<pre><code>log : Log
    #: The log object holding the results
</code></pre>
<h2 id="outputs">Outputs:</h2>
<pre><code>summaryLines : list[str]
    #: List of summary lines
resultLines : list[str]
    List of result lines
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def writeDuplicateDirsResult(log:Log):

    &#34;&#34;&#34;
        Description:
        ------------
        Write the results of the folder similarity duplicate search. Writes list of folder pairs + similarity scores and a short summary.

        
        Args:
        ----- 
            log : Log
                #: The log object holding the results
        
        Outputs:
        --------
            summaryLines : list[str]
                #: List of summary lines
            resultLines : list[str]
                List of result lines

    &#34;&#34;&#34;

    summaryLines = list()
    resultLines = list()
    sortedPairScores = SortPairScores(log)

    aboveThreshold = [pairScore for pairScore in sortedPairScores if pairScore[1] &gt;= log.config.similarityThreshold]

    summaryLines.append(&#34;Duplicate directory search results: \n&#34;)
    summaryLines.append(&#34;Scanned &#34; + str(len(sortedPairScores)) + &#34; directory pairs\n&#34; )
    summaryLines.append(&#34;Found &#34; + str(len(aboveThreshold)) + &#34; directory pairs with similarity above the threshold (&#34; + f&#34;{log.config.similarityThreshold*100:2.2f}%)\n&#34;)
    summaryLines.append(&#34;Found &#34; + str(len(log.allPairRecursiveScores)) + &#34; directory pairs with similar parents&#34;)

    resultLines.append(f&#34;\n{&#39;-&#39; * 100}\n{&#39; &#39; *35}Similar Parents\n{&#39;-&#39; * 100}\n&#34;)

    for basePair, similarParents in log.allPairRecursiveScores.items():
        resultLines.append(&#34;From pair: (&#34; + ParsePath(basePair[0]).as_posix() + &#34;, &#34; + ParsePath(basePair[1]).as_posix() +&#34;)&#34;)
        resultLines.append(&#34;Recursive search identified similar parents: &#34;)
        for parentPair in similarParents:
            resultLines.append(&#34;Pair: (&#34; + ParsePath(parentPair[0][0]).as_posix() + &#34;, &#34; + ParsePath(parentPair[0][1]).as_posix() +&#34;)&#34; + &#34; #: &#34; + &#34;Similarity: &#34; + f&#34;{parentPair[1]*100:2.2f}%\n&#34; )


    resultLines.append(f&#34;\n{&#39;-&#39; * 100}\n{&#39; &#39; *38}Folder pairs\n{&#39;-&#39; * 100}\n&#34;)


    resultLines.append(&#34;Pairs above threshold (&#34; + f&#34;{log.config.similarityThreshold*100:2.2f}%): \n&#34;)
    for pair, score in aboveThreshold:
        resultLines.append(&#34;Pair: (&#34; + ParsePath(pair[0]).as_posix() + &#34;, &#34; + ParsePath(pair[1]).as_posix() +&#34;)&#34; + &#34; #: &#34; + &#34;Similarity: &#34; + f&#34;{score*100:2.2f}%\n&#34; )
    resultLines.append(f&#34;\n{&#39;-&#39; * 100}\n&#34;)
    resultLines.append(&#34;Pairs below threshold: \n&#34;)
    for pair, score in sortedPairScores[len(aboveThreshold):]:
        resultLines.append(&#34;Pair: (&#34; + ParsePath(pair[0]).as_posix() + &#34;, &#34; + ParsePath(pair[1]).as_posix() +&#34;)&#34; + &#34; #: &#34; + &#34;Similarity:&#34; + f&#34;{score*100:2.2f}%&#34; )

    return summaryLines, resultLines</code></pre>
</details>
</dd>
<dt id="duplicates.write_duplicate_result_lines"><code class="name flex">
<span>def <span class="ident">write_duplicate_result_lines</span></span>(<span>log, iteration)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="description">Description:</h2>
<p>Write the results of the file duplicate search. Writes list of duplicated files (from each DupeFpLog) and the duplicated storage space.</p>
<h2 id="args">Args:</h2>
<pre><code>log : Log
    #: The log object holding the results
</code></pre>
<h2 id="outputs">Outputs:</h2>
<pre><code>summaryLines : list[str]
    #: List of summary lines
resultLines : list[str]
    List of result lines
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_duplicate_result_lines(log, iteration):
    &#34;&#34;&#34;
        Description:
        ------------
        Write the results of the file duplicate search. Writes list of duplicated files (from each DupeFpLog) and the duplicated storage space.

        
        Args:
        ----- 
            log : Log
                #: The log object holding the results
        
        Outputs:
        --------
            summaryLines : list[str]
                #: List of summary lines
            resultLines : list[str]
                List of result lines

    &#34;&#34;&#34;
    
    summaryLines = list()
    resultLines = list()

    summaryLines.append(&#34;File duplicate search summary: \n&#34;)
    summaryLines.append(&#34;Scanned &#34; + str(len(log.FpList)) + &#34; files (&#34;+str(size(log.totScannedStorage))+&#34;) &#34; + &#34;in &#34; + f&#34;{round(log.elapsedTime,2):00.00}&#34; +&#34;s / &#34; + f&#34;{round(log.elapsedTime/60,2):00.00}&#34; + &#34;m \n&#34; )
    summaryLines.append(&#34;Threshold: &#34; + size(log.config.sizeThreshold) + &#34; - Ignored &#34; + str(log.filesIgnored) + &#34; files \n&#34;)
    summaryLines.append(&#34;Excluded directories: &#34; + str([s for s in log.config.excludedPaths])+&#34;\n&#34;)
    summaryLines.append(&#34;Found &#34; + str(len(log.dupeFpList)) + &#34; duplicate fingerprints \n&#34; )
    
    if(log.totScannedStorage != 0): # To avoid div/0
        summaryLines.append(&#34;Total duplicated storage: &#34; + size(log.totDupeStorage) + &#34;     (&#34; + str(round(((log.totDupeStorage/log.totScannedStorage)*100), 1)) +&#34;%&#34; + &#34; of total)&#34;)
    

    resultLines.append(f&#34;\n{&#39;-&#39; * 100}\n{&#39; &#39; *45}Files\n{&#39;-&#39; * 100}\n&#34;)
    sortedDupeLogList = OrderDupeStorage(log)
    for dupeFpLog in sortedDupeLogList:
        # Duplicate files corresponding to this fingerprint
        dupeFiles = dupeFpLog.dupeFiles
        filesize, dupedStorage = dupeFpLog.fileSize, dupeFpLog.dupedStorage
        
        resultLines.append(&#34;File: (&#34; + size(filesize) + &#34;) - &#34; + str(len(dupeFiles)) + &#34; duplicates: \n&#34;)
        
        previousFolder = None
        for dupe in dupeFiles:
            if(dupe.path.parent == previousFolder):
                currentFolder = previousFolder
            else:
                currentFolder = dupe.path.parent
            # Write full path if writing a new folder
            if(currentFolder != previousFolder):
                resultLines.append(f&#34;{&#39; &#39;*6}&#34; + str(dupe.path.as_posix()) + &#34;\n&#34;)
            # Abbreviate the parent 
            else:
                resultLines.append(f&#34;{&#39; &#39;*6}&#34; + f&#34;{&#39; &#39;* (len(str(dupe.path.parent))-3)}&#34; +&#34;.../&#34;+ dupe.path.name + &#34;\n&#34;)
            previousFolder = currentFolder
        
        resultLines.append(&#34;\nDuplicated storage space: (&#34; + str(size(dupedStorage))+&#34;)\n&#34;)
        resultLines.append(&#34;Common root: &#34; + str(dupeFpLog.commonRoot.as_posix()) + &#34;\n &#34;)
        resultLines.append(f&#34;\n {&#39;-&#39; * 100}\n\n&#34;)
    return summaryLines, resultLines</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="duplicates.DupeFpLog"><code class="flex name class">
<span>class <span class="ident">DupeFpLog</span></span>
<span>(</span><span>Fp, log)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="class">Class:</h2>
<p>This class contains the information about a single duplicate fingerprint. Contains the fingerprint, a list of all
files that have this fingerprint and the corresponding file sizes (+ duplicated storage occupied) </p>
<h2 id="description">Description</h2>
<p>Constructor for the DupeFpLog class that takes the duplicated fingerprint string and a reference to the main log object.
Stores a list of all files with this duplicated fingerprint.
Args:</p>
<hr>
<pre><code>fp : str
    #: The duplicated fingerprint
log : Log
#: The log object for the duplicate search
</code></pre>
<h2 id="outputs">Outputs</h2>
<pre><code>allSizeExtClusters: list[list[File]]
    #: List containing all clusters
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DupeFpLog():
    &#39;&#39;&#39;
    Class:
    ------

    This class contains the information about a single duplicate fingerprint. Contains the fingerprint, a list of all 
    files that have this fingerprint and the corresponding file sizes (+ duplicated storage occupied) 
    &#39;&#39;&#39;
    
    def __init__(self, Fp, log):
        &#39;&#39;&#39; 
        Description
        -----------
        Constructor for the DupeFpLog class that takes the duplicated fingerprint string and a reference to the main log object.
        Stores a list of all files with this duplicated fingerprint.
        Args:
        -----
            fp : str
                #: The duplicated fingerprint
            log : Log
            #: The log object for the duplicate search

        Outputs
        -------
            allSizeExtClusters: list[list[File]]
                #: List containing all clusters
        &#39;&#39;&#39;
        # Reference to master log set after construction
        self.log = log
        # Fingerprint associated to this log
        self.Fp = Fp
        # List of duplicated files
        self.dupeFiles = log.FpFilesDict[Fp]
        # Maximal subtree root
        #self.commonRoot = findCommonRoot(self.dupeFiles)
        # Filesize and duplicated storage amount
        self.fileSize, self.dupedStorage = CalculateDuplicatedStorage(self.dupeFiles)</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fileorganiser" href="index.html">fileorganiser</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="duplicates.CalculateDuplicatedStorage" href="#duplicates.CalculateDuplicatedStorage">CalculateDuplicatedStorage</a></code></li>
<li><code><a title="duplicates.ClusterAllFiles" href="#duplicates.ClusterAllFiles">ClusterAllFiles</a></code></li>
<li><code><a title="duplicates.ClusterByExtension" href="#duplicates.ClusterByExtension">ClusterByExtension</a></code></li>
<li><code><a title="duplicates.ClusterBySize" href="#duplicates.ClusterBySize">ClusterBySize</a></code></li>
<li><code><a title="duplicates.FindDuplicateDirectories" href="#duplicates.FindDuplicateDirectories">FindDuplicateDirectories</a></code></li>
<li><code><a title="duplicates.FingerprintAllFiles" href="#duplicates.FingerprintAllFiles">FingerprintAllFiles</a></code></li>
<li><code><a title="duplicates.FingerprintAllFolders" href="#duplicates.FingerprintAllFolders">FingerprintAllFolders</a></code></li>
<li><code><a title="duplicates.GetFolderPairs" href="#duplicates.GetFolderPairs">GetFolderPairs</a></code></li>
<li><code><a title="duplicates.OrderDupeStorage" href="#duplicates.OrderDupeStorage">OrderDupeStorage</a></code></li>
<li><code><a title="duplicates.SizeClusterGen" href="#duplicates.SizeClusterGen">SizeClusterGen</a></code></li>
<li><code><a title="duplicates.SortPairScores" href="#duplicates.SortPairScores">SortPairScores</a></code></li>
<li><code><a title="duplicates.compareAllPairParents" href="#duplicates.compareAllPairParents">compareAllPairParents</a></code></li>
<li><code><a title="duplicates.compareFolderPairs" href="#duplicates.compareFolderPairs">compareFolderPairs</a></code></li>
<li><code><a title="duplicates.findDuplicatesMain" href="#duplicates.findDuplicatesMain">findDuplicatesMain</a></code></li>
<li><code><a title="duplicates.find_duplicate_fingerprints" href="#duplicates.find_duplicate_fingerprints">find_duplicate_fingerprints</a></code></li>
<li><code><a title="duplicates.fingerprintFile" href="#duplicates.fingerprintFile">fingerprintFile</a></code></li>
<li><code><a title="duplicates.fingerprintFolder" href="#duplicates.fingerprintFolder">fingerprintFolder</a></code></li>
<li><code><a title="duplicates.recursePairParents" href="#duplicates.recursePairParents">recursePairParents</a></code></li>
<li><code><a title="duplicates.store_duplicate_results" href="#duplicates.store_duplicate_results">store_duplicate_results</a></code></li>
<li><code><a title="duplicates.writeDuplicateDirsResult" href="#duplicates.writeDuplicateDirsResult">writeDuplicateDirsResult</a></code></li>
<li><code><a title="duplicates.write_duplicate_result_lines" href="#duplicates.write_duplicate_result_lines">write_duplicate_result_lines</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="duplicates.DupeFpLog" href="#duplicates.DupeFpLog">DupeFpLog</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>